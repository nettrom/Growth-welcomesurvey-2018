{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome Survey Aggregation\n",
    "\n",
    "We aggregate Welcome Survey responses from all wikis where the survey is deployed and store the data in two tables in Hive.\n",
    "\n",
    "Note that this notebook makes the following assumptions:\n",
    "\n",
    "1. The MediaWiki database is the authoritative source of welcome survey data.\n",
    "2. We identify responses by when they were made, not when the user registered.\n",
    "3. Users have to have registered in the same month as we are aggregating for, or some time in the previous month.\n",
    "4. We use the ServerSideAccountCreation schema to identify whether the user registered on the desktop or mobile site. If the user is not found in this schema, they are assumed to have registered on the desktop site.\n",
    "5. Known test accounts from Growth Team members are not counted.\n",
    "6. We are measuring *responses* to the Welcome Survey, not how accounts were created. That means that a user who responded to the survey is assumed to have seen it and been able to respond to it in a normal way (i.e. through the normal account creation process).\n",
    "\n",
    "As the Growth team expands coverage to more wikis, this notebook is set up to automatically discover and grab data from those wikis. This is done by getting the wiki configuration from WMFs servers and for each Wikipedia wiki checking if there's any Welcome Survey data stored in the database. If there is, the data gets processed and stored in the aggregate tables. Since this notebook is run once a month, this overhead of checking every Wikipedia isn't problematic.\n",
    "\n",
    "We also coalesce the aggregate tables at the end of the data update into a single file directly through Spark. This is done in order to speed up subsequent queries of the data.\n",
    "\n",
    "## FIXME\n",
    "\n",
    "Now that the Growth features are on all the wikis, it would be a lot faster to search through `ServerSideAccountCreation` *once* and then reuse that dataset, rather than query that dataset for every wiki. This worked great when we were on few wikis, and was designed to work great under those conditions. Now those conditions changes, and we should update. We could also create a view of the last two months of registrations and cache that view, then query that.\n",
    "\n",
    "Note that even with caching of ServerSideAccountCreation in Spark, we're still looking at about forty minutes of wall time to complete this. There's north of 200 wikis in the list, so that's around 10–12 seconds per wiki (which seems reasonable). I think having the caching is useful, but I also think it's important to change the timeout for the notebook to 2 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "from wmfdata import spark, mariadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration variables\n",
    "\n",
    "Let's define some configuration variables. First, the names of control and experiment groups we've used in Welcome Survey experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of survey groups and control groups\n",
    "control_groups = set(['exp1_group2', 'NONE'])\n",
    "survey_groups = set(['exp2_target_popup', 'exp1_group1', 'exp2_target_specialpage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, definitions of the questions in the survey. This dictionary maps the name of a question to either a set of possible responses, an iterator with possible responses, or an empty set. If a set or iterator of responses is defined those will be aggregated, and all non-matching responses aggregated as \"other\". If an empty set is defined, the script will figure out if the responses are a list and aggregate over it, otherwise it accepts any value as a valid response and aggregates over all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_specs = {\n",
    "    # \"Why did you create your account today?\"\n",
    "    'reason' : set([\n",
    "        'add-image',\n",
    "        'program-participant',\n",
    "        'placeholder',\n",
    "        'other',\n",
    "        'edit-info-add-change',\n",
    "        'read',\n",
    "        'edit-info',\n",
    "        'edit-typo',\n",
    "        'new-page'\n",
    "    ]),\n",
    "    # \"Have you ever edited Wikipedia?\"\n",
    "    'edited' : set([\n",
    "        'placeholder',\n",
    "        'yes-many',\n",
    "        'dont-remember',\n",
    "        'yes-few',\n",
    "        'no-other',\n",
    "        'dunno'\n",
    "    ]),\n",
    "    # Wikipedia is available in nearly 300 languages. Are there other languages you read and write in?\n",
    "    'languages' : set()\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## These were the autocompleted topis that we used for aggregating across topics.\n",
    "## This question was removed in September 2019\n",
    "topics = {\n",
    "    # Topics of interest. This set contains the autocompleted topics.\n",
    "    'topics' : set([\"entertainment\", \"food and drink\", \"biography\", \"military\",\n",
    "                    \"economics\", \"technology\", \"film\", \"philosophy\",\n",
    "                    \"business\", \"politics\", \"government\", \"engineering\",\n",
    "                    \"crafts and hobbies\", \"games\", \"health\", \"social science\",\n",
    "                    \"transportation\", \"education\"]),\n",
    "}\n",
    "## We asked users if they were interested in being mentored. That's been removed\n",
    "## since everyone has access to the mentor module.\n",
    "mentor = {\n",
    "    # Are you interested in being contacted to get help with editing?\n",
    "    'mentor' : [True, False],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database tables in Hive, we store overview counts (save/skip/abandon) in one table, and aggregates of the responses in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_table = 'growth_welcomesurvey.monthly_overview'\n",
    "response_table = 'growth_welcomesurvey.survey_response_aggregates'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the temporary view of ServerSideAccountCreation that we create, which will later be used to grab whether the registration happened on desktop or mobile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssac_temp_view = 'ssac_temp_view'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PHP files where MediaWiki language codes and language names are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_urls = [\n",
    "    \"https://raw.githubusercontent.com/wikimedia/mediawiki-extensions-cldr/master/CldrNames/CldrNamesEn.php\",\n",
    "    \"https://raw.githubusercontent.com/wikimedia/mediawiki-extensions-cldr/master/LocalNames/LocalNamesEn.php\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the create table statement used to create the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_overview_table_statement = '''\n",
    "CREATE TABLE {overview_table}\n",
    "(\n",
    "    wiki STRING COMMENT \"the wiki we gathered data for\",\n",
    "    log_month DATE COMMENT \"the month surveys were responded to (or rendered, if skipped)\",\n",
    "    platform STRING COMMENT \"the platform the user registered on (desktop or mobile)\",\n",
    "    user_group STRING COMMENT \"the user group the user was in (control or survey)\",\n",
    "    survey_response STRING COMMENT \"the response to the survey (save/skip/abandon)\",\n",
    "    num_responses INT COMMENT \"the number of responses\"\n",
    ")\n",
    "'''\n",
    "\n",
    "create_table_statement = '''\n",
    "CREATE TABLE {aggregate_table}\n",
    "(\n",
    "    wiki STRING COMMENT \"the wiki we gathered data for\",\n",
    "    log_month DATE COMMENT \"the month surveys were responded to (or rendered, if skipped)\",\n",
    "    platform STRING COMMENT \"the platform the user registered on (desktop or mobile)\",\n",
    "    q_name STRING COMMENT \"the name of the question (as used in the HTML form)\",\n",
    "    q_response STRING COMMENT \"the response to the question\",\n",
    "    num_responses INT COMMENT \"the number of responses\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(create_overview_table_statement.format(overview_table = overview_table))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(create_table_statement.format(aggregate_table = response_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Finding All Wikipedia Wikis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for retrieving wikis\n",
    "\n",
    "def get_dblist(list_name):\n",
    "    list_url = (\"https://noc.wikimedia.org/conf/dblists/\" + list_name + \".dblist\")\n",
    "    list_content = requests.get(list_url).text.split(\"\\n\")\n",
    "    return(pd.Series(list_content))\n",
    "\n",
    "def get_lang_names(url):\n",
    "    r = requests.get(url)\n",
    "    m = re.search(r\"languageNames = (\\[[\\s\\S]+?\\])\", r.text)\n",
    "    php_ln = m.group(1)\n",
    "    \n",
    "    json_ln = php_ln\n",
    "    repl = [\n",
    "        # Convert from PHP array format to JSON\n",
    "        (\" =>\", \":\"),\n",
    "        (\"\\[\", \"{\"),\n",
    "        (\"\\]\", \"}\"),\n",
    "        # Trailing commas will cause problems\n",
    "        (\",\\n}\", \"\\n}\"),\n",
    "        # ...so will single quotes\n",
    "        (\"'\", '\"'),\n",
    "        # ...and comments\n",
    "        (r\"/\\*[\\s\\S]*?\\*/\", \"\"),\n",
    "        (r\"#(.*?)\\n\", \"\"),\n",
    "        # One hack to deal with a single quote in a language name\n",
    "        ('O\"odham', \"O'odham\")\n",
    "    ]\n",
    "    for old, new in repl:\n",
    "        json_ln = re.sub(old, new, json_ln)\n",
    "    \n",
    "    py_ln = json.loads(json_ln)\n",
    "    return(py_ln)\n",
    "\n",
    "def apply_to_index(df, true_list, true_label, false_label):\n",
    "    idx_ser = df.index.to_series()\n",
    "    return(idx_ser.isin(true_list).apply(lambda x: true_label if x else false_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikis():\n",
    "    '''\n",
    "    Returns a `pandas.DataFrame` with an overview of all currently defined wikis,\n",
    "    their status (open/closed), and visibility (public/private).\n",
    "    '''\n",
    "    \n",
    "    wiki_query = '''\n",
    "SELECT\n",
    "    site_global_key AS database_code,\n",
    "    CONCAT(TRIM(LEADING \".\" FROM REVERSE(site_domain))) AS domain_name,\n",
    "    site_group AS database_group,\n",
    "    site_language AS language_code\n",
    "FROM enwiki.sites'''\n",
    "    \n",
    "    wikis = mariadb.run(wiki_query, 'enwiki').sort_values(\"database_code\").set_index(\"database_code\")\n",
    "    \n",
    "    langs = {}\n",
    "    for url in lang_urls:\n",
    "        langs.update(get_lang_names(url))\n",
    "\n",
    "    # Add languages not included in the CLDR files\n",
    "    langs.update({\n",
    "        \"als\": \"Alsatian\",\n",
    "        \"atj\": \"Atikamekw\",\n",
    "        \"diq\": \"Zazaki\",\n",
    "        \"fiu-vro\": \"Võro\",\n",
    "        \"map-bms\": \"Banyumasan\",\n",
    "        \"nah\": \"Nahuatl\",\n",
    "        \"pih\": \"Norfuk-Pitkern\",\n",
    "        \"rmy\": \"Vlax Romani\",\n",
    "        \"simple\": \"Simple English\"\n",
    "    })\n",
    "\n",
    "    wikis[\"language_name\"] = wikis[\"language_code\"].apply(langs.get)\n",
    "    \n",
    "    closed = get_dblist(\"closed\")\n",
    "    private = get_dblist(\"private\")\n",
    "    \n",
    "    wikis = (\n",
    "        wikis\n",
    "        .assign(\n",
    "            status=lambda df: apply_to_index(df, closed, \"closed\", \"open\"),\n",
    "            visbility=lambda df: apply_to_index(df, private, \"private\", \"public\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return(wikis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Known Users to Exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_known_users(wiki, username_patterns):\n",
    "    '''\n",
    "    For the given wiki, get the user IDs of accounts with a user name\n",
    "    matching any of the username patterns.\n",
    "    '''\n",
    "    \n",
    "    known_user_query = '''\n",
    "SELECT user_id\n",
    "FROM user\n",
    "WHERE user_name LIKE \"{name_pattern}%\"\n",
    "'''\n",
    "\n",
    "    known_users = set()\n",
    "    for u_pattern in username_patterns:\n",
    "        new_known = mariadb.run(known_user_query.format(\n",
    "            name_pattern = u_pattern), wiki)\n",
    "        known_users = known_users | set(new_known['user_id'])\n",
    "\n",
    "    # ok, done\n",
    "    return(known_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Username patterns of known test accounts, mainly Growth team members\n",
    "known_user_patterns = [\"MMiller\", \"Zilant\", \"Roan\", \"KHarlan\", \"MWang\", \"SBtest\",\n",
    "                       \"Cloud\", \"Rho2019\", \"KacemMhenni\", \"Test\"]\n",
    "\n",
    "## Set of known users, can be initialized with known accounts you want to make sure are ignored\n",
    "known_users = defaultdict(set)\n",
    "\n",
    "## Here are some known users from our four initial target wikis that we'll skip \n",
    "known_users['arwiki'].update([237660, 272774, 775023, 1175449, 1186377, 1506091, 1515147, 1538902,\n",
    "                      1568858, 1681813, 1683215, 1699418, 1699419, 1699425])\n",
    "known_users['cswiki'].update([303170, 342147, 349875, 44133, 100304, 307410, 439792, 444907, 454862,\n",
    "                      456272, 454003, 454846, 92295, 387915, 398470, 416764, 44751, 132801,\n",
    "                      137787, 138342, 268033, 275298, 317739, 320225, 328302, 339583, 341191,\n",
    "                      357559, 392634, 398626, 404765, 420805, 429109, 443890, 448195, 448438,\n",
    "                      453220, 453628, 453645, 453662, 453663, 453664, 440694, 427497, 272273,\n",
    "                      458025, 458487, 458049, 59563, 118067, 188859, 191908, 314640, 390445,\n",
    "                      451069, 459434, 460802, 460885])\n",
    "known_users['kowiki'].update([303170, 342147, 349875, 189097, 362732, 384066, 416362, 38759, 495265,\n",
    "                      515553, 537326, 566963, 567409, 416360, 414929, 470932, 472019, 485036,\n",
    "                      532123, 558423, 571587, 575553, 576758])\n",
    "known_users['viwiki'].update([451842, 628512, 628513, 680081, 680083, 680084, 680085, 680086, 355424,\n",
    "                      387563, 443216, 682713])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Get and Process Survey Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(month, wiki, known_user_set):\n",
    "    '''\n",
    "    Get all survey responses from the given wiki that were either submitted\n",
    "    or rendered (for abandoned responses) in the given month, excluding any\n",
    "    response from a user with a user ID in `known_user_set`\n",
    "    \n",
    "    :param month: (the date of the first day of) the month we are getting data for\n",
    "    :type month: datetime.date\n",
    "    \n",
    "    :param wiki: the database code of the wiki we are getting data for\n",
    "    :type wiki: str\n",
    "    \n",
    "    :param known_user_set: set of user IDs of known users that we exclude\n",
    "    :type known_user_set: set\n",
    "    \n",
    "    Returns a pandas.DataFrame with non-decoded JSON data.\n",
    "    '''\n",
    "\n",
    "    ## Get all Welcome Survey data where the response (skip/save)\n",
    "    ## was made in the given month, or the survey was rendered\n",
    "    ## in the given month (survey abandoned, or control group members).\n",
    "    ##\n",
    "    ## User has to have registered either in the same month or the previous month.\n",
    "    \n",
    "    ## Grabbing the first day of the previous month\n",
    "    prev_month = month - dt.timedelta(days = 1)\n",
    "    prev_month = prev_month.replace(day = 1)\n",
    "    \n",
    "    ## Getting the first day of the next month\n",
    "    if month.month == 12:\n",
    "        next_month = month.replace(year = month.year + 1, month = 1)\n",
    "    else:\n",
    "        next_month = month.replace(month = month.month + 1)\n",
    "\n",
    "    ws_data_query = '''\n",
    "SELECT\n",
    "    up_user AS user_id,\n",
    "    up_value AS survey_data\n",
    "FROM user_properties\n",
    "JOIN user\n",
    "ON up_user = user_id\n",
    "WHERE user_registration >= \"{prev_month_start}\"\n",
    "AND user_registration < \"{next_month_start}\"\n",
    "AND up_property = \"welcomesurvey-responses\"\n",
    "AND (\n",
    "        (\n",
    "         json_value(up_value, '$._submit_date') IS NULL -- not responded\n",
    "         AND CAST(json_value(up_value, '$._render_date') AS CHAR CHARACTER SET utf8)\n",
    "             REGEXP \"^{month}\") -- month of rendering matches\n",
    "    OR\n",
    "        (\n",
    "         CAST(json_value(up_value, '$._submit_date') AS CHAR CHARACTER SET utf8)\n",
    "             REGEXP \"^{month}\" -- month of submission matches\n",
    "        )\n",
    "    )\n",
    "'''\n",
    "    \n",
    "    if known_user_set:\n",
    "        ws_data_query = '''{query}\n",
    "        AND up_user NOT IN ({id_list})'''.format(\n",
    "            query = ws_data_query, id_list = ','.join([str(uid) for uid in known_user_set]))\n",
    "\n",
    "    responses = mariadb.run(\n",
    "        ws_data_query.format(\n",
    "            month = month.strftime('%Y%m'),\n",
    "            prev_month_start = prev_month.strftime('%Y%m%d000000'),\n",
    "            next_month_start = next_month.strftime('%Y%m%d000000')\n",
    "        ), wiki)\n",
    "    \n",
    "    return(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_mobile_flags(month, spark_session):\n",
    "    '''\n",
    "    Sets up a query of ServerSideAccountCreation to get registration data for the\n",
    "    month we're getting data for as well as the month prior, then caches the result.\n",
    "    Returns a cached Spark DataFrame that can later be queried for a specific wiki.\n",
    "    \n",
    "    :param month: (the date of the first day of) the month we are getting data for\n",
    "    :type month: datetime.date\n",
    "\n",
    "    :param spark_session: the PySpark session we're working with\n",
    "    :type spark_session: pyspark.SparkSession\n",
    "    '''\n",
    "    \n",
    "    ssac_query = '''\n",
    "SELECT\n",
    "    wiki AS wiki_db,\n",
    "    event.userid AS user_id,\n",
    "    CAST(event.displaymobile AS INT) AS reg_on_mobile\n",
    "FROM event_sanitized.serversideaccountcreation\n",
    "WHERE ((year = {prev_year} AND month = {prev_month})\n",
    "    OR (year = {cur_year} AND month = {cur_month}))\n",
    "    '''\n",
    "    \n",
    "    ## This expects `month` to be the first day of the month. It should be.\n",
    "    last_month = month - dt.timedelta(days = 1)\n",
    "    \n",
    "    ssac_df = spark_session.sql(\n",
    "        ssac_query.format(\n",
    "            prev_year = last_month.year,\n",
    "            prev_month = last_month.month,\n",
    "            cur_year = month.year,\n",
    "            cur_month = month.month\n",
    "        )\n",
    "    ).cache()\n",
    "    \n",
    "    return(ssac_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobile_flags(df, wiki_name):\n",
    "    '''\n",
    "    For all users who registered on the given wiki in the current or previous month,\n",
    "    get the displayMobile flag of their registration from the cached search of registrations.\n",
    "\n",
    "    `get_cached_mobile_flags()` sets up the caching.\n",
    "    \n",
    "    Returns only the user ID and whether the user registered on desktop or mobile,\n",
    "    as those are the two columns expected from `store_data()` because it's iterating\n",
    "    over each wiki.\n",
    "    \n",
    "    :param df: The cached DataFrame of ServerSideAccountCreation registration data\n",
    "    :type df: pyspark.sql.DataFrame\n",
    "\n",
    "    :param wiki_db: the database name of the wiki we are getting data for\n",
    "    :type wiki_db: str\n",
    "    '''\n",
    "\n",
    "    return(df.filter(df.wiki_db == wiki_name).select('user_id', 'reg_on_mobile').toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_responses(df):\n",
    "    '''\n",
    "    Process the survey responses found in the given DataFrame `df` and determine what\n",
    "    group (control or survey) the user was in, and whether they saved, skipped, or\n",
    "    abandoned the survey.\n",
    "    \n",
    "    Returns a `pandas.DataFrame` with five columns:\n",
    "    user id, desktop/mobile, user group, response (save/skip/abandon), \n",
    "    '''\n",
    "    \n",
    "    groups = []\n",
    "    userids = []\n",
    "    responses = []\n",
    "    render_timestamps = []\n",
    "    submit_timestamps = []\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        user_id = row.up_user\n",
    "        response = json.loads(row.up_value)\n",
    "        \n",
    "        userids.append(user_id)\n",
    "        \n",
    "        if response['_group'] == 'exp2_target_popup':\n",
    "            groups.append('C')\n",
    "        elif response['_group'] == 'exp2_target_specialpage':\n",
    "            groups.append('treatment')\n",
    "        elif response['_group'] == 'exp1_group1':\n",
    "            groups.append('target')\n",
    "        elif response['_group'] == 'exp1_group2':\n",
    "            groups.append('control')\n",
    "        elif response['_group'] == 'NONE':\n",
    "            groups.append('control')\n",
    "            \n",
    "        if not '_render_date' in response \\\n",
    "            or not response['_render_date']:\n",
    "            render_timestamps.append(None)\n",
    "        else:\n",
    "            render_timestamps.append(dt.datetime.strptime(response['_render_date'],\n",
    "                                                          '%Y%m%d%H%M%S'))\n",
    "            \n",
    "        if not '_submit_date' in response:\n",
    "            responses.append('abandon')\n",
    "            submit_timestamps.append(None)\n",
    "            continue\n",
    "        else:\n",
    "            submit_timestamps.append(dt.datetime.strptime(response['_submit_date'],\n",
    "                                                          '%Y%m%d%H%M%S'))\n",
    "            if '_skip' in response and response['_skip'] == True:\n",
    "                responses.append('skip')\n",
    "            else:\n",
    "                responses.append('save')\n",
    "            \n",
    "    return(pd.DataFrame(\n",
    "        {'group': groups,\n",
    "         'user_id': userids,\n",
    "         'response': responses,\n",
    "         'render_ts' : render_timestamps,\n",
    "         'submit_ts' : submit_timestamps}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toplines(rdf, c_groups, s_groups, wiki, month):\n",
    "    '''\n",
    "    Summarize the responses (which should be from a specific wiki and month)\n",
    "    and return the summaries as a dictionary that can be used in a query\n",
    "    to be inserted into Hive (or some other type of storage).\n",
    "    \n",
    "    :param rdf: survey responses to process\n",
    "    :type rdf: pandas.DataFrame\n",
    "    \n",
    "    :param c_groups: set of names of the control group(s) used\n",
    "    :type c_groups: set\n",
    "    \n",
    "    :param s_groups: set of names of the survey group(s) used\n",
    "    :type s_groups: set\n",
    "    \n",
    "    :param wiki: name of the wiki we're summarizing for\n",
    "    :type wiki: str\n",
    "    \n",
    "    :param month: first day of the month we're summarizing for\n",
    "    :type month: datetime.date\n",
    "    '''\n",
    "    \n",
    "    ## Columns in the output pandas.DataFrame\n",
    "    ## 1. wiki (str)\n",
    "    ## 2. month (datetime.date)\n",
    "    ## 3. platform (str, desktop/mobile)\n",
    "    ## 4. user group (str, survey/control)\n",
    "    ## 5. survey response (str, save/skip/abandon)\n",
    "    ## 6. number of responses (int)\n",
    "\n",
    "    ## Define the preferred order of the columns in the returned pandas.DataFrame\n",
    "    column_order = ['wiki', 'log_month', 'platform',\n",
    "                    'user_group', 'survey_response', 'num_responses']\n",
    "\n",
    "    user_ids = list() # used for counting\n",
    "    platforms = list()\n",
    "    user_groups = list()\n",
    "    responses = list()\n",
    "    \n",
    "    for row in rdf.itertuples():\n",
    "        user_ids.append(row.user_id)\n",
    "        \n",
    "        platform = 'desktop'\n",
    "        if(row.reg_on_mobile):\n",
    "            platform = 'mobile'\n",
    "        platforms.append(platform)\n",
    "        \n",
    "        response = json.loads(row.survey_data)\n",
    "        \n",
    "        if response['_group'] in c_groups:\n",
    "            user_groups.append('control')\n",
    "            responses.append('N/A')\n",
    "        elif response['_group'] in s_groups:\n",
    "            user_groups.append('survey')\n",
    "            if not '_submit_date' in response:\n",
    "                responses.append('abandon')\n",
    "            else:\n",
    "                if '_skip' in response and response['_skip'] == True:\n",
    "                    responses.append('skip')\n",
    "                else:\n",
    "                    responses.append('save')\n",
    "        else:\n",
    "            user_groups.append('unknown')\n",
    "            responses.append('N/A')\n",
    "        \n",
    "    res = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'platform' : platforms,\n",
    "        'user_group' : user_groups,\n",
    "        'survey_response' : responses\n",
    "    })\n",
    "    \n",
    "    res_agg = (res.groupby(['platform', 'user_group', 'survey_response'])\n",
    "               .agg({'user_id': 'count'})\n",
    "               .reset_index()\n",
    "               .rename(columns = {'user_id': 'num_responses'}))\n",
    "    \n",
    "    ## Add wiki and month columns\n",
    "    res_agg['wiki'] = wiki\n",
    "    res_agg['log_month'] = month\n",
    "    \n",
    "    ## Now, return the aggregate with the columns in the preferred order\n",
    "    return(res_agg[column_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(rdf, s_groups, q_name, q_responses, wiki, month):\n",
    "    '''\n",
    "    Process responses to a given question given by users in one of the survey groups.\n",
    "    If a user has entered a response that is invalid, we capture that and label\n",
    "    the response as 'other'.\n",
    "    \n",
    "    :param rdf: survey responses\n",
    "    :type rdf: pandas.DataFrame\n",
    "    \n",
    "    :param s_groups: set of names of the survey group(s) used, users not in those groups are skipped\n",
    "    :type s_groups: set\n",
    "    \n",
    "    :param q_name: name of the question (the key in the response dictionary)\n",
    "    :type q_name: str\n",
    "    \n",
    "    :param q_responses: valid responses to the question\n",
    "    :type q_responses: iterator\n",
    "    \n",
    "    :param wiki: name of the wiki we're summarizing for\n",
    "    :type wiki: str\n",
    "    \n",
    "    :param month: first day of the month we're summarizing for\n",
    "    :type month: datetime.date\n",
    "    '''\n",
    "    \n",
    "    ## Columns in the output pandas.DataFrame\n",
    "    ## 1. wiki (str)\n",
    "    ## 2. month (datetime.date)\n",
    "    ## 3. platform (str, desktop/mobile)\n",
    "    ## 4. question name\n",
    "    ## 5. question response\n",
    "    ## 6. number of responses (int)\n",
    "    \n",
    "    ## Define the preferred order of the columns in the returned pandas.DataFrame\n",
    "    column_order = ['wiki', 'log_month', 'platform',\n",
    "                    'q_name', 'q_response', 'num_responses']\n",
    "   \n",
    "    user_ids = list() # used for counting\n",
    "    platforms = list()\n",
    "    names = list()\n",
    "    responses = list()\n",
    "    \n",
    "    for row in rdf.itertuples():\n",
    "        response = json.loads(row.survey_data)\n",
    "        \n",
    "        ## only counting responses from survey group users\n",
    "        if response['_group'] not in s_groups:\n",
    "            continue\n",
    "        \n",
    "        ## only counting users who saved the survey\n",
    "        if '_skip' in response or not '_submit_date' in response:\n",
    "            continue\n",
    "        \n",
    "        ## We have four possibilities here:\n",
    "        ## 1: the question name isn't in the response: we record a \"N/A\"\n",
    "        ## 2: the response is a list: we ignore the config and record the entire list of answers\n",
    "        ## 3: we have a set of defined responses: we check against it and record \"other\" if no match\n",
    "        ## 4: otherwise, we record the response and keep going\n",
    "        \n",
    "        try:\n",
    "            res = response[q_name]\n",
    "            if isinstance(res, list):\n",
    "                cur_response = res\n",
    "            elif q_responses:\n",
    "                if res in q_responses:\n",
    "                    cur_response = [res]\n",
    "                else:\n",
    "                    cur_response = ['other']\n",
    "            else:\n",
    "                cur_response = [res]\n",
    "        except KeyError:\n",
    "            cur_response = ['N/A']\n",
    "\n",
    "        ## Note: we make all these into a list of length 1, because we can then multiply them\n",
    "        ## to make their length match the length of the response:\n",
    "\n",
    "        cur_user_id = [row.user_id]\n",
    "        cur_platform = ['desktop']\n",
    "        if(row.reg_on_mobile):\n",
    "            cur_platform = ['mobile']\n",
    "        cur_q_name = [q_name]\n",
    "\n",
    "        ## Et voila!\n",
    "        user_ids.extend(cur_user_id * len(cur_response))\n",
    "        platforms.extend(cur_platform * len(cur_response))\n",
    "        names.extend(cur_q_name * len(cur_response))\n",
    "        responses.extend(cur_response)\n",
    "        \n",
    "    res = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'platform' : platforms,\n",
    "        'q_name' : names,\n",
    "        'q_response' : responses\n",
    "    })\n",
    "    \n",
    "    res_agg = (res.groupby(['platform', 'q_name', 'q_response'])\n",
    "               .agg({'user_id': 'count'})\n",
    "               .reset_index()\n",
    "               .rename(columns = {'user_id': 'num_responses'}))\n",
    "    \n",
    "    ## The survey responses are defined to be strings, so we enforce that data type\n",
    "    res_agg['q_response'] = res_agg['q_response'].astype('string')\n",
    "    \n",
    "    ## Add wiki and month columns\n",
    "    res_agg['wiki'] = wiki\n",
    "    res_agg['log_month'] = month\n",
    "    \n",
    "    ## Now, return the aggregate with the columns in the preferred order\n",
    "    return(res_agg[column_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main function to store data for all wikis for a given month\n",
    "\n",
    "def store_data(month, wikis, known_users,\n",
    "               control_groups, survey_groups, q_config, overview_table, aggregate_response_table):\n",
    "    '''\n",
    "    Process survey responses from the given `month` on every wiki in `wikis`, excluding\n",
    "    responses from test accounts (and other non-relevant respondents). Test accounts can\n",
    "    be defined by wiki and page ID in `known_users` (a defaultdict of set).\n",
    "    \n",
    "    :param month: the first day of the month we're getting data for (e.g. 2019-11-01)\n",
    "    :type month: datetime.date\n",
    "    \n",
    "    :param known_users: dictionary mapping wiki to a set of known users by user ID\n",
    "    :type known_users: collections.defaultdict\n",
    "    \n",
    "    :param control_groups: names of the control groups used in the data\n",
    "    :type control_groups: set\n",
    "    \n",
    "    :param survey_groups: names of the survey groups used in the data\n",
    "    :type survey_groups: set\n",
    "    \n",
    "    :param q_config: mapping of survey questions to valid responses\n",
    "                    (see previous documentation earlier in the notebook)\n",
    "    :type q_config: dict\n",
    "    \n",
    "    :param overview_table: name of the database table in which to store the monthly toplines\n",
    "    :type overview_table: str\n",
    "    \n",
    "    :param aggregate_response_table: name of the database table in which to store the\n",
    "                                     per-question aggregated responses\n",
    "    :type aggregate_response_table: str\n",
    "    '''\n",
    "    \n",
    "    ## Grab the PySpark session that we'll use for querying ServerSideAccountCreation\n",
    "    ## and for writing data to the Data Lake\n",
    "    spark_session = spark.get_session()\n",
    "\n",
    "    ## Set up the cached query of ServerSideAccountCreation\n",
    "    ssac_cached_df = get_cached_mobile_flags(month, spark_session)\n",
    "\n",
    "    for wiki in wikis:\n",
    "        try:\n",
    "            known_users[wiki] | get_known_users(wiki, known_user_patterns)\n",
    "        except:\n",
    "            print('Unable to grab a list of known users from {}: {}'.format(wiki, sys.exc_info()[0]))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response_data = get_responses(month, wiki, known_users[wiki])\n",
    "        except:\n",
    "            print('Unable to retrieve responses from {}: {}'.format(wiki, sys.exc_info()[0]))\n",
    "            continue\n",
    "        \n",
    "        ## Skip this wiki if we had no responses\n",
    "        if response_data.empty:\n",
    "            continue\n",
    "\n",
    "        ## Get mobile flags, merge, those who are not found are assumed to be\n",
    "        ## on the desktop site since we fill with 0.\n",
    "        response_data = response_data.merge(\n",
    "            get_mobile_flags(ssac_cached_df, wiki), how = 'left', on = 'user_id').fillna(0)\n",
    "        \n",
    "        ## Because of the left join reg_on_mobile becomes a float, so let's force it to be int\n",
    "        response_data['reg_on_mobile'] = response_data['reg_on_mobile'].astype(int)\n",
    "        \n",
    "        ## Get the toplines\n",
    "        toplines = get_toplines(response_data, control_groups, survey_groups, wiki, month)\n",
    "        \n",
    "        ## This takes the toplines DF, turns it into a Spark DataFrame\n",
    "        ## then does an INSERT INTO the overview table of the data in that DataFrame.\n",
    "        toplines_sdf = spark_session.createDataFrame(toplines)\n",
    "        toplines_sdf.write.insertInto(overview_table)\n",
    "\n",
    "        ## Iterate over the configuration and extract responses\n",
    "        aggregated_responses = pd.DataFrame()\n",
    "        for q_name, q_responses in q_config.items():\n",
    "            aggregated_responses = pd.concat(\n",
    "                [aggregated_responses,\n",
    "                 get_q(response_data, survey_groups, q_name, q_responses, wiki, month)\n",
    "                ])\n",
    "        \n",
    "        ## Similarly as for toplines, convert into a Spark DataFrame and then\n",
    "        ## insert the data into the table with aggregated responses.\n",
    "        if not aggregated_responses.empty:\n",
    "            aggregated_responses_sdf = spark_session.createDataFrame(aggregated_responses)\n",
    "            aggregated_responses_sdf.write.insertInto(aggregate_response_table)\n",
    "        \n",
    "        print('aggregated responses for {}'.format(wiki))\n",
    "          \n",
    "    # ok, done\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce_table(table_name, n_partitions = 1):\n",
    "    '''\n",
    "    This function takes the given table and changes the number of Hive partitions\n",
    "    it uses. It does this by reading the table contents, calling pyspark's coalesce() to\n",
    "    change the number of partitions, and writing the result out into to a temporary table.\n",
    "    It will then delete and overwrite the original table. It should be used with\n",
    "    caution because the optimal number of partitions depends on the size and\n",
    "    structure of your dataset.\n",
    "    '''\n",
    "    \n",
    "    # make a temp table name based on today's date\n",
    "    today = dt.datetime.now(dt.timezone.utc).date()\n",
    "    temp_table_name = \"{}_temp_{}\".format(table_name, today.strftime(\"%Y%m%d\"))\n",
    "    \n",
    "    # grab a spark session\n",
    "    spark_session = spark.get_session()\n",
    "    # read in the data table, coalesce it, then write it to the temp table\n",
    "    spark_session.read.table(table_name).coalesce(n_partitions).write.saveAsTable(temp_table_name)\n",
    "\n",
    "    # We refresh the table as otherwise Spark think its stored in a different file and throws\n",
    "    # a FileNotFoundException when we try to read and overwrite the original table\n",
    "    spark_session.catalog.refreshTable(temp_table_name)\n",
    "\n",
    "    try:\n",
    "        # Now we can overwrite the original table and refresh that too, so hopefully nothing goes wrong\n",
    "        spark_session.read.table(temp_table_name).write.saveAsTable(table_name, mode = 'overwrite')\n",
    "        spark_session.catalog.refreshTable(table_name)\n",
    "    except:\n",
    "        logging.error(f'unable to overwrite table \"{table_name}\", keeping \"{temp_table_name}\" as backup')\n",
    "    else:\n",
    "        try:\n",
    "            spark.run(f'DROP TABLE {temp_table_name}')\n",
    "        except UnboundLocalError:\n",
    "            # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "            # and so we ignore that error\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The date range in the code below is _inclusive_, so the last month will also be run. Therefore, no overlap between a current run and an earlier run is necessary. A bunch of old data pulls are kept here for reference so it's clear when we pulled data, which wikis we ran it for, and so on."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for month in pd.date_range(dt.date(2018, 11, 1), dt.date(2018, 12, 1), freq = \"MS\"):\n",
    "    store_data(month.date(), ['cswiki', 'kowiki'],\n",
    "           known_users, known_user_patterns, control_groups, survey_groups, filenames)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for month in pd.date_range(dt.date(2019, 1, 1), dt.date(2019, 6, 1), freq = \"MS\"):\n",
    "    store_data(month.date(), ['cswiki', 'kowiki', 'viwiki'],\n",
    "           known_users, known_user_patterns, control_groups, survey_groups, filenames)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for month in pd.date_range(dt.date(2019, 7, 1), dt.date(2019, 11, 1), freq = \"MS\"):\n",
    "    store_data(month.date(), ['cswiki', 'kowiki', 'arwiki'],\n",
    "           known_users, known_user_patterns, control_groups, survey_groups, filenames)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for month in pd.date_range(dt.date(2019, 12, 1), dt.date(2020, 2, 1), freq = \"MS\"):\n",
    "    store_data(month.date(), ['cswiki', 'kowiki', 'arwiki'],\n",
    "           known_users, known_user_patterns, control_groups, survey_groups, filenames)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Data pull in March 2021, in preparation for reducing the data retention window.\n",
    "## Grabbing data from July 2020 through February 2021.\n",
    "for month in pd.date_range(dt.date(2020, 7, 1), dt.date(2021, 2, 1), freq = \"MS\"):\n",
    "    store_data(month.date(), wikis,\n",
    "           known_users, control_groups, survey_groups, question_specs, overview_table, response_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We expect this notebook to be run once a month through a cron job (or equivalent)\n",
    "## during the month _after_ which we're grabbing data. We'll grab today's date,\n",
    "## change it to the first of the month, walk back one day to get the previous month.\n",
    "## We then set the date to the first of that month because that's what we use as `log_month`.\n",
    "\n",
    "today = dt.datetime.now(dt.timezone.utc).date()\n",
    "first_of_this_month = today.replace(day = 1)\n",
    "last_of_previous_month = first_of_this_month - dt.timedelta(days = 1)\n",
    "first_of_previous_month = last_of_previous_month.replace(day = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab an updated list of all wikis, then limit it to all open and public\n",
    "## Wikipeda wikis:\n",
    "\n",
    "all_wikis = get_wikis()\n",
    "all_wikipedia_wikis = all_wikis.loc[(all_wikis['database_group'] == 'wikipedia') &\n",
    "                                    (all_wikis['status'] == 'open') &\n",
    "                                    (all_wikis['visbility'] == 'public')].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated responses for afwiki\n",
      "aggregated responses for akwiki\n",
      "aggregated responses for alswiki\n",
      "aggregated responses for altwiki\n",
      "aggregated responses for amwiki\n",
      "aggregated responses for anwiki\n",
      "aggregated responses for arwiki\n",
      "aggregated responses for arywiki\n",
      "aggregated responses for arzwiki\n",
      "aggregated responses for astwiki\n",
      "aggregated responses for aswiki\n",
      "aggregated responses for atjwiki\n",
      "aggregated responses for avkwiki\n",
      "aggregated responses for aywiki\n",
      "aggregated responses for azbwiki\n",
      "aggregated responses for azwiki\n",
      "aggregated responses for barwiki\n",
      "aggregated responses for bawiki\n",
      "aggregated responses for bclwiki\n",
      "aggregated responses for be_x_oldwiki\n",
      "aggregated responses for bewiki\n",
      "aggregated responses for bgwiki\n",
      "aggregated responses for bhwiki\n",
      "aggregated responses for biwiki\n",
      "aggregated responses for bmwiki\n",
      "aggregated responses for bnwiki\n",
      "aggregated responses for bowiki\n",
      "aggregated responses for bpywiki\n",
      "aggregated responses for brwiki\n",
      "aggregated responses for bswiki\n",
      "aggregated responses for bugwiki\n",
      "aggregated responses for bxrwiki\n",
      "aggregated responses for cawiki\n",
      "aggregated responses for cebwiki\n",
      "aggregated responses for cewiki\n",
      "aggregated responses for chrwiki\n",
      "aggregated responses for chywiki\n",
      "aggregated responses for ckbwiki\n",
      "aggregated responses for cowiki\n",
      "aggregated responses for crhwiki\n",
      "aggregated responses for crwiki\n",
      "aggregated responses for csbwiki\n",
      "aggregated responses for cswiki\n",
      "aggregated responses for cuwiki\n",
      "aggregated responses for cywiki\n",
      "aggregated responses for dagwiki\n",
      "aggregated responses for dawiki\n",
      "aggregated responses for dewiki\n",
      "aggregated responses for dvwiki\n",
      "aggregated responses for dzwiki\n",
      "aggregated responses for eewiki\n",
      "aggregated responses for elwiki\n",
      "aggregated responses for emlwiki\n",
      "aggregated responses for enwiki\n",
      "aggregated responses for eowiki\n",
      "aggregated responses for eswiki\n",
      "aggregated responses for etwiki\n",
      "aggregated responses for euwiki\n",
      "aggregated responses for fawiki\n",
      "aggregated responses for ffwiki\n",
      "aggregated responses for fiu_vrowiki\n",
      "aggregated responses for fiwiki\n",
      "aggregated responses for fowiki\n",
      "aggregated responses for frwiki\n",
      "aggregated responses for furwiki\n",
      "aggregated responses for gagwiki\n",
      "aggregated responses for gawiki\n",
      "aggregated responses for gcrwiki\n",
      "aggregated responses for gdwiki\n",
      "aggregated responses for glwiki\n",
      "aggregated responses for gnwiki\n",
      "aggregated responses for gomwiki\n",
      "aggregated responses for guwiki\n",
      "aggregated responses for gvwiki\n",
      "aggregated responses for hawiki\n",
      "aggregated responses for hewiki\n",
      "aggregated responses for hifwiki\n",
      "aggregated responses for hiwiki\n",
      "aggregated responses for hrwiki\n",
      "aggregated responses for htwiki\n",
      "aggregated responses for huwiki\n",
      "aggregated responses for hywiki\n",
      "aggregated responses for hywwiki\n",
      "aggregated responses for idwiki\n",
      "aggregated responses for igwiki\n",
      "aggregated responses for ikwiki\n",
      "aggregated responses for inhwiki\n",
      "aggregated responses for iswiki\n",
      "aggregated responses for itwiki\n",
      "aggregated responses for iuwiki\n",
      "aggregated responses for jawiki\n",
      "aggregated responses for jvwiki\n",
      "aggregated responses for kaawiki\n",
      "aggregated responses for kawiki\n",
      "aggregated responses for kgwiki\n",
      "aggregated responses for kiwiki\n",
      "aggregated responses for kkwiki\n",
      "aggregated responses for klwiki\n",
      "aggregated responses for kmwiki\n",
      "aggregated responses for knwiki\n",
      "aggregated responses for koiwiki\n",
      "aggregated responses for kowiki\n",
      "aggregated responses for kswiki\n",
      "aggregated responses for kuwiki\n",
      "aggregated responses for kwwiki\n",
      "aggregated responses for kywiki\n",
      "aggregated responses for lawiki\n",
      "aggregated responses for lbwiki\n",
      "aggregated responses for lfnwiki\n",
      "aggregated responses for lgwiki\n",
      "aggregated responses for lijwiki\n",
      "aggregated responses for lldwiki\n",
      "aggregated responses for lmowiki\n",
      "aggregated responses for lnwiki\n",
      "aggregated responses for lowiki\n",
      "aggregated responses for ltwiki\n",
      "aggregated responses for lvwiki\n",
      "aggregated responses for maiwiki\n",
      "aggregated responses for map_bmswiki\n",
      "aggregated responses for mgwiki\n",
      "aggregated responses for mhrwiki\n",
      "aggregated responses for minwiki\n",
      "aggregated responses for miwiki\n",
      "aggregated responses for mkwiki\n",
      "aggregated responses for mlwiki\n",
      "aggregated responses for mniwiki\n",
      "aggregated responses for mnwiki\n",
      "aggregated responses for mnwwiki\n",
      "Unable to grab a list of known users from mowiki: <class 'ValueError'>\n",
      "aggregated responses for mrwiki\n",
      "aggregated responses for mswiki\n",
      "aggregated responses for mtwiki\n",
      "aggregated responses for mwlwiki\n",
      "aggregated responses for mywiki\n",
      "aggregated responses for mznwiki\n",
      "aggregated responses for nawiki\n",
      "aggregated responses for nds_nlwiki\n",
      "aggregated responses for ndswiki\n",
      "aggregated responses for newiki\n",
      "aggregated responses for nlwiki\n",
      "aggregated responses for nnwiki\n",
      "aggregated responses for novwiki\n",
      "aggregated responses for nowiki\n",
      "aggregated responses for nqowiki\n",
      "aggregated responses for nsowiki\n",
      "aggregated responses for nywiki\n",
      "aggregated responses for ocwiki\n",
      "aggregated responses for olowiki\n",
      "aggregated responses for omwiki\n",
      "aggregated responses for orwiki\n",
      "aggregated responses for pagwiki\n",
      "aggregated responses for pamwiki\n",
      "aggregated responses for papwiki\n",
      "aggregated responses for pawiki\n",
      "aggregated responses for pcdwiki\n",
      "aggregated responses for pflwiki\n",
      "aggregated responses for pihwiki\n",
      "aggregated responses for piwiki\n",
      "aggregated responses for plwiki\n",
      "aggregated responses for pswiki\n",
      "aggregated responses for ptwiki\n",
      "aggregated responses for quwiki\n",
      "aggregated responses for rmwiki\n",
      "aggregated responses for rmywiki\n",
      "aggregated responses for rnwiki\n",
      "aggregated responses for roa_rupwiki\n",
      "aggregated responses for rowiki\n",
      "aggregated responses for ruwiki\n",
      "aggregated responses for rwwiki\n",
      "aggregated responses for sahwiki\n",
      "aggregated responses for satwiki\n",
      "aggregated responses for sawiki\n",
      "aggregated responses for scnwiki\n",
      "aggregated responses for scowiki\n",
      "aggregated responses for scwiki\n",
      "aggregated responses for sdwiki\n",
      "aggregated responses for sgwiki\n",
      "aggregated responses for shwiki\n",
      "aggregated responses for simplewiki\n",
      "aggregated responses for siwiki\n",
      "aggregated responses for skrwiki\n",
      "aggregated responses for skwiki\n",
      "aggregated responses for slwiki\n",
      "aggregated responses for smwiki\n",
      "aggregated responses for snwiki\n",
      "aggregated responses for sowiki\n",
      "aggregated responses for sqwiki\n",
      "aggregated responses for srnwiki\n",
      "aggregated responses for srwiki\n",
      "aggregated responses for sswiki\n",
      "aggregated responses for stwiki\n",
      "aggregated responses for suwiki\n",
      "aggregated responses for svwiki\n",
      "aggregated responses for swwiki\n",
      "aggregated responses for szlwiki\n",
      "aggregated responses for tawiki\n",
      "aggregated responses for tcywiki\n",
      "aggregated responses for tewiki\n",
      "aggregated responses for tgwiki\n",
      "aggregated responses for thwiki\n",
      "aggregated responses for tiwiki\n",
      "aggregated responses for tkwiki\n",
      "aggregated responses for tlwiki\n",
      "aggregated responses for tnwiki\n",
      "aggregated responses for tpiwiki\n",
      "aggregated responses for trwiki\n",
      "aggregated responses for tswiki\n",
      "aggregated responses for ttwiki\n",
      "aggregated responses for tumwiki\n",
      "aggregated responses for twwiki\n",
      "aggregated responses for tyvwiki\n",
      "aggregated responses for udmwiki\n",
      "aggregated responses for ugwiki\n",
      "aggregated responses for ukwiki\n",
      "aggregated responses for urwiki\n",
      "aggregated responses for uzwiki\n",
      "aggregated responses for vecwiki\n",
      "aggregated responses for vewiki\n",
      "aggregated responses for vlswiki\n",
      "aggregated responses for warwiki\n",
      "aggregated responses for wowiki\n",
      "aggregated responses for wuuwiki\n",
      "aggregated responses for xalwiki\n",
      "aggregated responses for xhwiki\n",
      "aggregated responses for yiwiki\n",
      "aggregated responses for zawiki\n",
      "aggregated responses for zeawiki\n",
      "aggregated responses for zh_classicalwiki\n",
      "aggregated responses for zh_min_nanwiki\n",
      "aggregated responses for zh_yuewiki\n",
      "aggregated responses for zuwiki\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_data(first_of_previous_month,\n",
    "           all_wikipedia_wikis['database_code'],\n",
    "           known_users,\n",
    "           control_groups,\n",
    "           survey_groups,\n",
    "           question_specs,\n",
    "           overview_table,\n",
    "           response_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n",
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "coalesce_table(overview_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n",
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "coalesce_table(response_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
